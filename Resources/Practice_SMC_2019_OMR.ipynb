{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEIQQXr_-dsr"
      },
      "source": [
        "# Sound and Music Computing Summer School 2019\n",
        "## Practice session on Optical Music Recognition\n",
        "\n",
        "**Speaker**: Jorge Calvo-Zaragoza (jcalvo@dlsi.ua.es)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEH3PubvZVK"
      },
      "source": [
        "# Google Colaboratory\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the (Google) cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser.\n",
        "\n",
        "\n",
        "## Getting started\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that prints a greeting and a universal truth:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShZB7Ms5vcfr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "533f606b-ffef-4db7-8feb-7fd94b348e09"
      },
      "source": [
        "print('Hello SMC,')\n",
        "print('OMR is cool!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello SMC,\n",
            "OMR is cool!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETyHK9EYvkcU"
      },
      "source": [
        "We can also execute console commands by placing *!* at the beggining of the line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzR0UXN5vkvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "715c489b-26fd-4bff-ae72-f18debdebbad"
      },
      "source": [
        "!/usr/bin/nvidia-smi\n",
        "\n",
        "!python --version "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 26 12:28:04 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Python 3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tpEVKOcrTlx"
      },
      "source": [
        "# Keras\n",
        "\n",
        "[Keras](https://keras.io/) is a high-level API for neural networks, obviously including deep learning (DL). Actually, it is main motivation. \n",
        "\n",
        "It is written in Python and runs over [TensorFlow](https://www.tensorflow.org/) (Google), [CNTK ](https://www.microsoft.com/en-us/cognitive-toolkit/) (Microsoft), [Theano](http://deeplearning.net/software/theano/) or [MxNet](https://mxnet.incubator.apache.org/) (Amazon) as backbone frameworks. It was primarily developed for research uses, with the aim of being able to move rapidly from an idea to preliminary results in the shortest possible time.\n",
        "\n",
        "Essentially, Keras:\n",
        "\n",
        "- allows fast prototyping because of its simplicity, modularity and extensibility.\n",
        "- supports the most typical neural network elements (units, training algorithms, loss functions, etc.)\n",
        "- runs over both CPU and GPU.\n",
        "\n",
        "Most interesting charasteristics:\n",
        "\n",
        "- **Ease of use**: Keras is an API designed for human beings. Keras follows best practices to reduce cognitive load: it offers consistent and simple APIs, minimizes the number of user actions required for common use cases, and provides clear and actionable feedback about user error.\n",
        "\n",
        "- **Modularity**: A model is understood as a sequence or graphic of independent modules, totally configurable, that can be connected to each other with the minimum possible constraints. In particular, the neural layers, the cost functions, the optimizers, the initialization schemes, the activation functions and the regularization schemes are independent modules that can be combined to create new experiments.\n",
        "\n",
        "- **Allows easy extension**: New modules are easy to add (like new classes and functions), and existing modules provide wide examples. Being able to easily create new modules allows total expressivity, making Keras suitable for advanced research.\n",
        "\n",
        "- **Work with Python**: There are no separate configuration for neural models. The models are directly described in the Python code, which is compact and easy-to-debug."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7gPxPC7rRJb"
      },
      "source": [
        "# Step-by-step Optical Music Recognition system\n",
        "\n",
        "Let's build a simple, yet complete, staff-based Optical Music Recognition (OMR) system.\n",
        "\n",
        "The system will make use of deep neural networks, organized in an encoder-decoder topology based on Recurrent Neural Networks (RNN). Our encoder will also include a Convolutional Neural Network (CNN) to perform the feature extraction process during the encoding stage.\n",
        "\n",
        "## Input domain\n",
        "\n",
        "We need to define the input domain of our OMR system.\n",
        "\n",
        "We choose an easy one:\n",
        "\n",
        "* Printed\n",
        "* Ideal conditions\n",
        "* Modern notation\n",
        "* Monophonic\n",
        "\n",
        "We'll see that it is easy to extend this to other domains.\n",
        "\n",
        "## Output domain\n",
        "\n",
        "Sequence of **agnostic** symbols.\n",
        "\n",
        "## Data\n",
        "\n",
        "It is well known that Deep Learning models require training sets of sufficient size. The [Printed Images of Music Staves](https://grfia.dlsi.ua.es/primus/) (PrIMuS) dataset was devised to fulfill these requirements for end-to-end staff-based OMR research. Thus, the objective pursued when creating this ground-truth data was not to represent the most complex musical notation corpus, but collect the highest possible number of scores readily available to be represented in formats suitable for OMR experimentation and evaluation.\n",
        "\n",
        "PrIMuS contains 87678 real-music incipits (an incipit is a sequence of notes, typically the first ones, used for identifying a melody or musical work), each one represented by five files: the Plaine and Easie code source, an image with the rendered score, the musical symbolic representation of the incipit both in Music Encoding Initiative format (MEI) and in an on-purpose simplified encoding (semantic encoding), and a sequence containing the graphical symbols shown in the score with their position in the staff without any musical meaning (agnostic encoding). \n",
        "\n",
        "A sample from PrIMuS:\n",
        "\n",
        "![Sample image](https://github.com/calvozaragoza/tf-deep-omr/blob/master/Data/Example/000051652-1_2_1.png?raw=true)\n",
        "\n",
        "whose *agnostic* representation sequence is as follows:\n",
        "\n",
        "`clef.C-L1\taccidental.flat-L4\taccidental.flat-L2\taccidental.flat-S3\tdigit.2-L4\tdigit.4-L2\tdigit.2-S5 digit.3-S5\tmultirest-L3\tbarline-L1\trest.quarter-L3\trest.eighth-L3\tnote.eighth-L4\tbarline-L1 note.quarter-L4\tdot-S4\tnote.eighth-L3\tbarline-L1\tnote.quarter-S5\tdot-S5\tnote.eighth-L5\tbarline-L1 note.eighth-S4\tnote.eighth-S4\trest.quarter-L3\tbarline-L1`\n",
        "\n",
        "Due to efficiency reasons, we will just use a subset of PrIMuS for this tutorial. We will build our training set of pairs (image, agnostic representation) to train our deep encoder-decoder architecture. \n",
        "\n",
        "\n",
        "## Model overview \n",
        "\n",
        "Our DL model will consist of:\n",
        "\n",
        "* Encoder CNN: the input image is first fed into a CNN that learns a feature representation that is useful for the task at hand.\n",
        "* Encoder RNN: the last filtered image of the CNN is used as input to the encoder RNN, that processes the image frame by frame to produce a latent representation of the whole input staff region\n",
        "* Decoder RNN: from the latent vector, the decoder RNN will predict a symbol step by step. The last predicted symbol is fed into the decoder in the next step. The first symbol provided to the encoder is that of *start of sentence* (SOS). The process is stopped once the decoder RNN predicts an *end of sentence* (EOS) symbol.\n",
        "\n",
        "A graphical overview of the considered architecture is shown in the following figure:\n",
        "\n",
        "![Topology](https://drive.google.com/uc?id=14EVEPy-zedcPBYyavefiAeeBcpG6J0HZ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEt6pluBxssh"
      },
      "source": [
        "## Preliminary: generators\n",
        "\n",
        "In the case of tasks solved with RNNs, and especially with the encoder-decoder approach, it is usual to have variable-length inputs and outputs sequences. Although, by definition, RNNs can handle sequences of variable size, internally Keras (likewise other frameworks) works with fixed-dimension data batches for a more efficient processing.\n",
        "\n",
        "\n",
        "To alleviate this problem, there are two alternatives: to create as many batches as sequences are (which is inefficient in training) or to use the *padding* technique. Padding consists in calculating the longest sequence and establishing the dimensions according to this value. Shorter sequences are filled with null values (typically 0). The problem is that, when the differences between the lengths are very high, padding becomes very severe and causes a less effective learning.\n",
        "\n",
        "\n",
        "However, there is an intermediate, elegant, solution to deal with this issue: mini batches. That is, to build small batches and apply intra-batch padding. To be able to do this in Keras, it is necessary to use *generator* functions. Generators are Python functions that prepare such batches. On each call, the generator prepares the next batch to be considered and *freezes*, until it is called in again to generate another batch. Generator functions allow us to return batches that contain a fixed number of elements. Then we can do padding at the batch level, thereby reducing the aforementioned problem.\n",
        "\n",
        "\n",
        "The following code provides an intuitive example of this behavior (pay special attention to keywords **yield** and **next**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD6QurPPxtpv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cad27b3e-7469-4b02-8f78-94627604d215"
      },
      "source": [
        "import numpy as np    # Python's Matrix package\n",
        "\n",
        "sample = [[1,2], [3,4,5], [6,7,8,9], [10]]\n",
        "\n",
        "def example_generator(data, batch_size):  \n",
        "    while True:\n",
        "      for idx in range(0,len(data),batch_size):\n",
        "        batch = data[idx:idx+batch_size]                \n",
        "        max_length = max([len(b) for b in batch])        \n",
        "        fixed_dim_batch = np.zeros((len(batch),max_length), dtype=np.int)\n",
        "        \n",
        "        for i,b in enumerate(batch):\n",
        "          fixed_dim_batch[i,:len(b)] = b\n",
        "          \n",
        "        yield fixed_dim_batch\n",
        "                          \n",
        "generator = example_generator(sample, batch_size = 2)\n",
        "\n",
        "print('Batch',1)\n",
        "x  = next(generator)\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print()\n",
        "print('Batch',2)\n",
        "x  = next(generator)\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1\n",
            "[[1 2 0]\n",
            " [3 4 5]]\n",
            "(2, 3)\n",
            "\n",
            "Batch 2\n",
            "[[ 6  7  8  9]\n",
            " [10  0  0  0]]\n",
            "(2, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsMEDQKUx-Nd"
      },
      "source": [
        "### Bucketting\n",
        "\n",
        "Another concept (which will not appear in the implementation below) is *bucketting*. This strategy first orders the dataset so that elements within the same batch are of similar length. This minimizes intra-batch padding, thus approaching the optimal (but inefficient) way of performing the training process element by element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF5sdY651B7z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "44753e02-60ca-42d9-f927-53c8acb5e43d"
      },
      "source": [
        "import numpy as np    # Python's Matrix package\n",
        "\n",
        "sample = [[1,2], [3,4,5], [6,7,8,9], [10]]\n",
        "\n",
        "def bucketting_generator(data, batch_size):\n",
        "    data.sort(key = len)\n",
        "    while True:\n",
        "      for idx in range(0,len(data),batch_size):\n",
        "        batch = data[idx:idx+batch_size]                \n",
        "        max_length = max([len(b) for b in batch])        \n",
        "        fixed_dim_batch = np.zeros((len(batch),max_length), dtype=np.int)\n",
        "        \n",
        "        for i,b in enumerate(batch):\n",
        "          fixed_dim_batch[i,:len(b)] = b\n",
        "          \n",
        "        yield fixed_dim_batch\n",
        "                          \n",
        "generator = bucketting_generator(sample, batch_size = 2)\n",
        "\n",
        "print('Batch',1)\n",
        "x  = next(generator)\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print()\n",
        "print('Batch',2)\n",
        "x  = next(generator)\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1\n",
            "[[10  0]\n",
            " [ 1  2]]\n",
            "(2, 2)\n",
            "\n",
            "Batch 2\n",
            "[[3 4 5 0]\n",
            " [6 7 8 9]]\n",
            "(2, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1X7sOxxnLou"
      },
      "source": [
        "# Code\n",
        "\n",
        "## Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bRR4WMXbAXf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d68f229-10c0-497c-953a-2fa2e29af0c5"
      },
      "source": [
        "\"\"\"\n",
        "Initialization of Google Drive file system\n",
        "\"\"\"\n",
        "\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Cursos/SMC Summer School 2019/PriMuS-40000.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()\n",
        "\n",
        "data_folder = '/tmp/PriMuS-40000'\n",
        "num_samples = len(glob.glob(data_folder+'/*.png'))\n",
        "\n",
        "print('Num. samples:',num_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Num. samples: 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvKvhEupZ4z0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f506f53f-cd79-4074-c409-1e7f514fc510"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "Data loading:\n",
        "\n",
        "Each sample consists of a pair (x,y) such that:\n",
        " - x: single-staff image (*.png)\n",
        " - y: sequence of symbols (*.txt)\n",
        " \n",
        "\"\"\"\n",
        "\n",
        "import cv2     # OpenCV image package\n",
        "import tqdm    # Loop with loading bar\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "num_samples = 20000 # Tune according to resources\n",
        "\n",
        "# Loop over each image file\n",
        "for idx in tqdm.tqdm(range(1,num_samples+1)):\n",
        "  image = cv2.imread(data_folder+'/'+str(idx)+'.png', False) # Grayscale\n",
        "  \n",
        "  sequence_file = open(data_folder+'/'+str(idx)+'.txt', \"r\")\n",
        "  sequence = sequence_file.readline().split()\n",
        "  sequence_file.close()\n",
        "  \n",
        "  X.append(image)\n",
        "  Y.append(sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:29<00:00, 689.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEGRn9Pa9iBX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "2cd443a2-073d-4f3c-d70d-991f64889ef6"
      },
      "source": [
        "\"\"\"\n",
        "Visualization of data loading\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.dpi']= 150\n",
        "\n",
        "idx_example = 5\n",
        "\n",
        "# Display first two samples\n",
        "f = plt.figure()\n",
        "f.add_subplot(1,1,1)\n",
        "plt.imshow(X[idx_example])\n",
        "plt.show(block=True)\n",
        "\n",
        "print(Y[idx_example])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxEAAACICAYAAACY5mZwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5P8BNcmvVCFBHKkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['clef.F-L4', 'accidental.sharp-L4', 'accidental.sharp-S2', 'metersign.C-L3', 'digit.1-S5', 'multirest-L3', 'barline-L1', 'rest.quarter-L3', 'rest.eighth-L3', 'note.sixteenth-S3', 'note.sixteenth-L4', 'note.sixteenth-S4', 'note.eighth-S4', 'barline-L1', 'rest.half-L3', 'rest.quarter-L3', 'rest.eighth-L3', 'note.eighth-S4', 'barline-L1', 'note.eighth-L4', 'note.eighth-L4', 'rest.eighth-L3', 'note.eighth-L5', 'note.quarter-L4', 'rest.eighth-L3', 'note.eighth-L5', 'barline-L1', 'note.eighth-L5', 'note.eighth-L3', 'rest.sixteenth-L3', 'note.sixteenth-L3', 'note.sixteenth-L3', 'note.sixteenth-S3', 'note.quarter-L4']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbYt9St6nQs-"
      },
      "source": [
        "\"\"\"\n",
        "The network needs a fixed height (i.e., features per frame).\n",
        "The width is variable, and so we rescale to a fixed height\n",
        "but keeping the aspect ratio.\n",
        "\"\"\"\n",
        "\n",
        "def resize(image, height):\n",
        "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
        "    return cv2.resize(image, (width, height))\n",
        "\n",
        "img_height = 64\n",
        "for idx,image in enumerate(X):\n",
        "  X[idx] = resize(image,img_height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFwpYQwj1mTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2bc5b200-3bfb-417c-9136-f4d87ded4f66"
      },
      "source": [
        "\"\"\"\n",
        "We need the special characters 'start of sequence' (sos) and\n",
        "'end of sequence' (eos)\n",
        "\"\"\" \n",
        "output_sos = '<s>'\n",
        "output_eos = '</s>'\n",
        "\n",
        "# We include both to the ground-truth sequence\n",
        "Y = [ [ output_sos ] +  sequence + [ output_eos ] for sequence in Y]\n",
        "\n",
        "# We can then compute the set of symbols\n",
        "alphabet = set()\n",
        "\n",
        "for sequence in Y:\n",
        "    alphabet.update(sequence)\n",
        "              \n",
        "alphabet_len = len(alphabet)    \n",
        "\n",
        "print('There is a total of ' + str(alphabet_len) + ' symbols: ' + str(sorted(alphabet)))\n",
        "\n",
        "# Building conversors from symbol to int and inversely\n",
        "alphabet_from_char_to_int = dict([(char, i) for i, char in enumerate(alphabet)])\n",
        "alphabet_from_int_to_char = dict([(i, char) for i, char in enumerate(alphabet)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is a total of 706 symbols: ['</s>', '<s>', 'accidental.flat-L-1', 'accidental.flat-L0', 'accidental.flat-L1', 'accidental.flat-L2', 'accidental.flat-L3', 'accidental.flat-L4', 'accidental.flat-L5', 'accidental.flat-L6', 'accidental.flat-L7', 'accidental.flat-L8', 'accidental.flat-S-1', 'accidental.flat-S-2', 'accidental.flat-S0', 'accidental.flat-S1', 'accidental.flat-S2', 'accidental.flat-S3', 'accidental.flat-S4', 'accidental.flat-S5', 'accidental.flat-S6', 'accidental.flat-S7', 'accidental.natural-L-1', 'accidental.natural-L-2', 'accidental.natural-L0', 'accidental.natural-L1', 'accidental.natural-L2', 'accidental.natural-L3', 'accidental.natural-L4', 'accidental.natural-L5', 'accidental.natural-L6', 'accidental.natural-L7', 'accidental.natural-L8', 'accidental.natural-S-1', 'accidental.natural-S-2', 'accidental.natural-S-3', 'accidental.natural-S0', 'accidental.natural-S1', 'accidental.natural-S2', 'accidental.natural-S3', 'accidental.natural-S4', 'accidental.natural-S5', 'accidental.natural-S6', 'accidental.natural-S7', 'accidental.sharp-L-1', 'accidental.sharp-L-2', 'accidental.sharp-L0', 'accidental.sharp-L1', 'accidental.sharp-L2', 'accidental.sharp-L3', 'accidental.sharp-L4', 'accidental.sharp-L5', 'accidental.sharp-L6', 'accidental.sharp-L7', 'accidental.sharp-S-1', 'accidental.sharp-S-2', 'accidental.sharp-S0', 'accidental.sharp-S1', 'accidental.sharp-S2', 'accidental.sharp-S3', 'accidental.sharp-S4', 'accidental.sharp-S5', 'accidental.sharp-S6', 'accidental.sharp-S7', 'barline-L1', 'clef.C-L1', 'clef.C-L2', 'clef.C-L3', 'clef.C-L4', 'clef.F-L3', 'clef.F-L4', 'clef.G-L1', 'clef.G-L2', 'digit.0-S5', 'digit.1-L2', 'digit.1-S5', 'digit.11-L4', 'digit.12-L4', 'digit.16-L2', 'digit.2-L2', 'digit.2-L4', 'digit.2-S5', 'digit.24-L4', 'digit.3-L2', 'digit.3-L4', 'digit.3-S5', 'digit.4-L2', 'digit.4-L4', 'digit.4-S5', 'digit.5-L4', 'digit.5-S5', 'digit.6-L2', 'digit.6-L4', 'digit.6-S5', 'digit.7-S5', 'digit.8-L2', 'digit.8-L4', 'digit.8-S5', 'digit.9-L4', 'digit.9-S5', 'dot-S-1', 'dot-S-2', 'dot-S-3', 'dot-S0', 'dot-S1', 'dot-S2', 'dot-S3', 'dot-S4', 'dot-S5', 'dot-S6', 'dot-S7', 'dot-S8', 'fermata.above-S6', 'gracenote.beamedBoth1-L1', 'gracenote.beamedBoth1-L2', 'gracenote.beamedBoth1-L3', 'gracenote.beamedBoth1-L4', 'gracenote.beamedBoth1-L5', 'gracenote.beamedBoth1-L6', 'gracenote.beamedBoth1-S0', 'gracenote.beamedBoth1-S1', 'gracenote.beamedBoth1-S2', 'gracenote.beamedBoth1-S3', 'gracenote.beamedBoth1-S4', 'gracenote.beamedBoth1-S5', 'gracenote.beamedBoth2-L-1', 'gracenote.beamedBoth2-L0', 'gracenote.beamedBoth2-L1', 'gracenote.beamedBoth2-L2', 'gracenote.beamedBoth2-L3', 'gracenote.beamedBoth2-L4', 'gracenote.beamedBoth2-L5', 'gracenote.beamedBoth2-L6', 'gracenote.beamedBoth2-S0', 'gracenote.beamedBoth2-S1', 'gracenote.beamedBoth2-S2', 'gracenote.beamedBoth2-S3', 'gracenote.beamedBoth2-S4', 'gracenote.beamedBoth2-S5', 'gracenote.beamedBoth3-L2', 'gracenote.beamedBoth3-L3', 'gracenote.beamedBoth3-L4', 'gracenote.beamedBoth3-L5', 'gracenote.beamedBoth3-L6', 'gracenote.beamedBoth3-S2', 'gracenote.beamedBoth3-S3', 'gracenote.beamedBoth3-S4', 'gracenote.eighth-L-1', 'gracenote.eighth-L0', 'gracenote.eighth-L1', 'gracenote.eighth-L2', 'gracenote.eighth-L3', 'gracenote.eighth-L4', 'gracenote.eighth-L5', 'gracenote.eighth-L6', 'gracenote.eighth-S-1', 'gracenote.eighth-S0', 'gracenote.eighth-S1', 'gracenote.eighth-S2', 'gracenote.eighth-S3', 'gracenote.eighth-S4', 'gracenote.eighth-S5', 'gracenote.half-L2', 'gracenote.half-S2', 'gracenote.quarter-L1', 'gracenote.quarter-L2', 'gracenote.quarter-L3', 'gracenote.quarter-L4', 'gracenote.quarter-L5', 'gracenote.quarter-L6', 'gracenote.quarter-S0', 'gracenote.quarter-S1', 'gracenote.quarter-S2', 'gracenote.quarter-S3', 'gracenote.quarter-S4', 'gracenote.quarter-S5', 'gracenote.sixteenth-L-1', 'gracenote.sixteenth-L0', 'gracenote.sixteenth-L1', 'gracenote.sixteenth-L2', 'gracenote.sixteenth-L3', 'gracenote.sixteenth-L4', 'gracenote.sixteenth-L5', 'gracenote.sixteenth-L6', 'gracenote.sixteenth-S-1', 'gracenote.sixteenth-S-2', 'gracenote.sixteenth-S0', 'gracenote.sixteenth-S1', 'gracenote.sixteenth-S2', 'gracenote.sixteenth-S3', 'gracenote.sixteenth-S4', 'gracenote.sixteenth-S5', 'gracenote.thirty_second-L-1', 'gracenote.thirty_second-L0', 'gracenote.thirty_second-L1', 'gracenote.thirty_second-L2', 'gracenote.thirty_second-L3', 'gracenote.thirty_second-L4', 'gracenote.thirty_second-L5', 'gracenote.thirty_second-L6', 'gracenote.thirty_second-S-1', 'gracenote.thirty_second-S-2', 'gracenote.thirty_second-S0', 'gracenote.thirty_second-S1', 'gracenote.thirty_second-S2', 'gracenote.thirty_second-S3', 'gracenote.thirty_second-S4', 'gracenote.thirty_second-S5', 'metersign.C-L3', 'metersign.C/-L3', 'multirest-L3', 'note.beamedBoth0-L0', 'note.beamedBoth0-L1', 'note.beamedBoth0-L2', 'note.beamedBoth0-L3', 'note.beamedBoth0-L4', 'note.beamedBoth0-L5', 'note.beamedBoth0-L6', 'note.beamedBoth0-L7', 'note.beamedBoth0-S-1', 'note.beamedBoth0-S-3', 'note.beamedBoth0-S0', 'note.beamedBoth0-S1', 'note.beamedBoth0-S2', 'note.beamedBoth0-S3', 'note.beamedBoth0-S4', 'note.beamedBoth0-S5', 'note.beamedBoth0-S6', 'note.beamedBoth1-L-1', 'note.beamedBoth1-L-2', 'note.beamedBoth1-L-3', 'note.beamedBoth1-L0', 'note.beamedBoth1-L1', 'note.beamedBoth1-L2', 'note.beamedBoth1-L3', 'note.beamedBoth1-L4', 'note.beamedBoth1-L5', 'note.beamedBoth1-L6', 'note.beamedBoth1-L7', 'note.beamedBoth1-L8', 'note.beamedBoth1-S-1', 'note.beamedBoth1-S-2', 'note.beamedBoth1-S-3', 'note.beamedBoth1-S0', 'note.beamedBoth1-S1', 'note.beamedBoth1-S2', 'note.beamedBoth1-S3', 'note.beamedBoth1-S4', 'note.beamedBoth1-S5', 'note.beamedBoth1-S6', 'note.beamedBoth1-S7', 'note.beamedBoth1-S8', 'note.beamedBoth2-L-1', 'note.beamedBoth2-L-2', 'note.beamedBoth2-L-3', 'note.beamedBoth2-L0', 'note.beamedBoth2-L1', 'note.beamedBoth2-L2', 'note.beamedBoth2-L3', 'note.beamedBoth2-L4', 'note.beamedBoth2-L5', 'note.beamedBoth2-L6', 'note.beamedBoth2-L7', 'note.beamedBoth2-L8', 'note.beamedBoth2-S-1', 'note.beamedBoth2-S-2', 'note.beamedBoth2-S-3', 'note.beamedBoth2-S0', 'note.beamedBoth2-S1', 'note.beamedBoth2-S2', 'note.beamedBoth2-S3', 'note.beamedBoth2-S4', 'note.beamedBoth2-S5', 'note.beamedBoth2-S6', 'note.beamedBoth2-S7', 'note.beamedBoth2-S8', 'note.beamedBoth3-L-1', 'note.beamedBoth3-L-2', 'note.beamedBoth3-L0', 'note.beamedBoth3-L1', 'note.beamedBoth3-L2', 'note.beamedBoth3-L3', 'note.beamedBoth3-L4', 'note.beamedBoth3-L5', 'note.beamedBoth3-L6', 'note.beamedBoth3-L7', 'note.beamedBoth3-L8', 'note.beamedBoth3-S-1', 'note.beamedBoth3-S-2', 'note.beamedBoth3-S0', 'note.beamedBoth3-S1', 'note.beamedBoth3-S2', 'note.beamedBoth3-S3', 'note.beamedBoth3-S4', 'note.beamedBoth3-S5', 'note.beamedBoth3-S6', 'note.beamedBoth3-S7', 'note.beamedBoth3-S8', 'note.beamedBoth4-L-1', 'note.beamedBoth4-L0', 'note.beamedBoth4-L1', 'note.beamedBoth4-L2', 'note.beamedBoth4-L3', 'note.beamedBoth4-L4', 'note.beamedBoth4-L5', 'note.beamedBoth4-L6', 'note.beamedBoth4-L7', 'note.beamedBoth4-S-1', 'note.beamedBoth4-S0', 'note.beamedBoth4-S1', 'note.beamedBoth4-S2', 'note.beamedBoth4-S3', 'note.beamedBoth4-S4', 'note.beamedBoth4-S5', 'note.beamedBoth4-S6', 'note.beamedBoth4-S7', 'note.beamedBoth5-L0', 'note.beamedBoth5-L1', 'note.beamedLeft0-L1', 'note.beamedLeft0-L2', 'note.beamedLeft0-L3', 'note.beamedLeft0-L4', 'note.beamedLeft0-L5', 'note.beamedLeft0-L6', 'note.beamedLeft0-L7', 'note.beamedLeft0-S-2', 'note.beamedLeft0-S1', 'note.beamedLeft0-S2', 'note.beamedLeft0-S3', 'note.beamedLeft0-S4', 'note.beamedLeft0-S5', 'note.beamedLeft1-L-1', 'note.beamedLeft1-L-2', 'note.beamedLeft1-L0', 'note.beamedLeft1-L1', 'note.beamedLeft1-L2', 'note.beamedLeft1-L3', 'note.beamedLeft1-L4', 'note.beamedLeft1-L5', 'note.beamedLeft1-L6', 'note.beamedLeft1-L7', 'note.beamedLeft1-L8', 'note.beamedLeft1-S-1', 'note.beamedLeft1-S-2', 'note.beamedLeft1-S-3', 'note.beamedLeft1-S0', 'note.beamedLeft1-S1', 'note.beamedLeft1-S2', 'note.beamedLeft1-S3', 'note.beamedLeft1-S4', 'note.beamedLeft1-S5', 'note.beamedLeft1-S6', 'note.beamedLeft1-S7', 'note.beamedLeft1-S8', 'note.beamedLeft2-L-1', 'note.beamedLeft2-L-2', 'note.beamedLeft2-L0', 'note.beamedLeft2-L1', 'note.beamedLeft2-L2', 'note.beamedLeft2-L3', 'note.beamedLeft2-L4', 'note.beamedLeft2-L5', 'note.beamedLeft2-L6', 'note.beamedLeft2-L7', 'note.beamedLeft2-L8', 'note.beamedLeft2-S-1', 'note.beamedLeft2-S-2', 'note.beamedLeft2-S-3', 'note.beamedLeft2-S0', 'note.beamedLeft2-S1', 'note.beamedLeft2-S2', 'note.beamedLeft2-S3', 'note.beamedLeft2-S4', 'note.beamedLeft2-S5', 'note.beamedLeft2-S6', 'note.beamedLeft2-S7', 'note.beamedLeft2-S8', 'note.beamedLeft3-L-1', 'note.beamedLeft3-L-2', 'note.beamedLeft3-L0', 'note.beamedLeft3-L1', 'note.beamedLeft3-L2', 'note.beamedLeft3-L3', 'note.beamedLeft3-L4', 'note.beamedLeft3-L5', 'note.beamedLeft3-L6', 'note.beamedLeft3-L7', 'note.beamedLeft3-L8', 'note.beamedLeft3-S-1', 'note.beamedLeft3-S-2', 'note.beamedLeft3-S0', 'note.beamedLeft3-S1', 'note.beamedLeft3-S2', 'note.beamedLeft3-S3', 'note.beamedLeft3-S4', 'note.beamedLeft3-S5', 'note.beamedLeft3-S6', 'note.beamedLeft3-S7', 'note.beamedLeft3-S8', 'note.beamedLeft4-L0', 'note.beamedLeft4-L1', 'note.beamedLeft4-L2', 'note.beamedLeft4-L3', 'note.beamedLeft4-L4', 'note.beamedLeft4-L5', 'note.beamedLeft4-L6', 'note.beamedLeft4-S0', 'note.beamedLeft4-S1', 'note.beamedLeft4-S2', 'note.beamedLeft4-S3', 'note.beamedLeft4-S4', 'note.beamedLeft4-S5', 'note.beamedLeft4-S6', 'note.beamedLeft5-S0', 'note.beamedLeft5-S1', 'note.beamedRight0-L0', 'note.beamedRight0-L1', 'note.beamedRight0-L2', 'note.beamedRight0-L3', 'note.beamedRight0-L4', 'note.beamedRight0-L5', 'note.beamedRight0-L6', 'note.beamedRight0-L7', 'note.beamedRight0-S0', 'note.beamedRight0-S1', 'note.beamedRight0-S2', 'note.beamedRight0-S3', 'note.beamedRight0-S4', 'note.beamedRight0-S5', 'note.beamedRight0-S6', 'note.beamedRight0-S7', 'note.beamedRight1-L-1', 'note.beamedRight1-L-2', 'note.beamedRight1-L-3', 'note.beamedRight1-L0', 'note.beamedRight1-L1', 'note.beamedRight1-L2', 'note.beamedRight1-L3', 'note.beamedRight1-L4', 'note.beamedRight1-L5', 'note.beamedRight1-L6', 'note.beamedRight1-L7', 'note.beamedRight1-L8', 'note.beamedRight1-S-1', 'note.beamedRight1-S-2', 'note.beamedRight1-S-3', 'note.beamedRight1-S0', 'note.beamedRight1-S1', 'note.beamedRight1-S2', 'note.beamedRight1-S3', 'note.beamedRight1-S4', 'note.beamedRight1-S5', 'note.beamedRight1-S6', 'note.beamedRight1-S7', 'note.beamedRight1-S8', 'note.beamedRight2-L-1', 'note.beamedRight2-L-2', 'note.beamedRight2-L-3', 'note.beamedRight2-L0', 'note.beamedRight2-L1', 'note.beamedRight2-L2', 'note.beamedRight2-L3', 'note.beamedRight2-L4', 'note.beamedRight2-L5', 'note.beamedRight2-L6', 'note.beamedRight2-L7', 'note.beamedRight2-L8', 'note.beamedRight2-S-1', 'note.beamedRight2-S-2', 'note.beamedRight2-S-3', 'note.beamedRight2-S0', 'note.beamedRight2-S1', 'note.beamedRight2-S2', 'note.beamedRight2-S3', 'note.beamedRight2-S4', 'note.beamedRight2-S5', 'note.beamedRight2-S6', 'note.beamedRight2-S7', 'note.beamedRight2-S8', 'note.beamedRight3-L-1', 'note.beamedRight3-L0', 'note.beamedRight3-L1', 'note.beamedRight3-L2', 'note.beamedRight3-L3', 'note.beamedRight3-L4', 'note.beamedRight3-L5', 'note.beamedRight3-L6', 'note.beamedRight3-L7', 'note.beamedRight3-L8', 'note.beamedRight3-S-1', 'note.beamedRight3-S-2', 'note.beamedRight3-S0', 'note.beamedRight3-S1', 'note.beamedRight3-S2', 'note.beamedRight3-S3', 'note.beamedRight3-S4', 'note.beamedRight3-S5', 'note.beamedRight3-S6', 'note.beamedRight3-S7', 'note.beamedRight3-S8', 'note.beamedRight4-L0', 'note.beamedRight4-L3', 'note.beamedRight4-L4', 'note.beamedRight4-L5', 'note.beamedRight4-L6', 'note.beamedRight4-L7', 'note.beamedRight4-S-1', 'note.beamedRight4-S-2', 'note.beamedRight4-S0', 'note.beamedRight4-S1', 'note.beamedRight4-S2', 'note.beamedRight4-S3', 'note.beamedRight4-S4', 'note.beamedRight4-S7', 'note.double_whole-L1', 'note.double_whole-L2', 'note.double_whole-L3', 'note.double_whole-L4', 'note.double_whole-L5', 'note.double_whole-L6', 'note.double_whole-L7', 'note.double_whole-S0', 'note.double_whole-S2', 'note.double_whole-S3', 'note.double_whole-S4', 'note.double_whole-S5', 'note.double_whole-S6', 'note.double_whole-S7', 'note.eighth-L-1', 'note.eighth-L-2', 'note.eighth-L-3', 'note.eighth-L0', 'note.eighth-L1', 'note.eighth-L2', 'note.eighth-L3', 'note.eighth-L4', 'note.eighth-L5', 'note.eighth-L6', 'note.eighth-L7', 'note.eighth-L8', 'note.eighth-S-1', 'note.eighth-S-2', 'note.eighth-S-3', 'note.eighth-S0', 'note.eighth-S1', 'note.eighth-S2', 'note.eighth-S3', 'note.eighth-S4', 'note.eighth-S5', 'note.eighth-S6', 'note.eighth-S7', 'note.eighth-S8', 'note.half-L-1', 'note.half-L-2', 'note.half-L-3', 'note.half-L0', 'note.half-L1', 'note.half-L2', 'note.half-L3', 'note.half-L4', 'note.half-L5', 'note.half-L6', 'note.half-L7', 'note.half-L8', 'note.half-S-1', 'note.half-S-2', 'note.half-S-3', 'note.half-S0', 'note.half-S1', 'note.half-S2', 'note.half-S3', 'note.half-S4', 'note.half-S5', 'note.half-S6', 'note.half-S7', 'note.half-S8', 'note.quadruple_whole-L5', 'note.quadruple_whole-S3', 'note.quarter-L-1', 'note.quarter-L-2', 'note.quarter-L-3', 'note.quarter-L0', 'note.quarter-L1', 'note.quarter-L2', 'note.quarter-L3', 'note.quarter-L4', 'note.quarter-L5', 'note.quarter-L6', 'note.quarter-L7', 'note.quarter-L8', 'note.quarter-S-1', 'note.quarter-S-2', 'note.quarter-S-3', 'note.quarter-S0', 'note.quarter-S1', 'note.quarter-S2', 'note.quarter-S3', 'note.quarter-S4', 'note.quarter-S5', 'note.quarter-S6', 'note.quarter-S7', 'note.quarter-S8', 'note.sixteenth-L-1', 'note.sixteenth-L-2', 'note.sixteenth-L0', 'note.sixteenth-L1', 'note.sixteenth-L2', 'note.sixteenth-L3', 'note.sixteenth-L4', 'note.sixteenth-L5', 'note.sixteenth-L6', 'note.sixteenth-L7', 'note.sixteenth-L8', 'note.sixteenth-S-1', 'note.sixteenth-S-2', 'note.sixteenth-S0', 'note.sixteenth-S1', 'note.sixteenth-S2', 'note.sixteenth-S3', 'note.sixteenth-S4', 'note.sixteenth-S5', 'note.sixteenth-S6', 'note.sixteenth-S7', 'note.sixteenth-S8', 'note.thirty_second-L-1', 'note.thirty_second-L0', 'note.thirty_second-L1', 'note.thirty_second-L2', 'note.thirty_second-L3', 'note.thirty_second-L4', 'note.thirty_second-L5', 'note.thirty_second-L6', 'note.thirty_second-L7', 'note.thirty_second-S0', 'note.thirty_second-S1', 'note.thirty_second-S2', 'note.thirty_second-S3', 'note.thirty_second-S4', 'note.thirty_second-S5', 'note.thirty_second-S6', 'note.whole-L-1', 'note.whole-L0', 'note.whole-L1', 'note.whole-L2', 'note.whole-L3', 'note.whole-L4', 'note.whole-L5', 'note.whole-L6', 'note.whole-L7', 'note.whole-S-1', 'note.whole-S0', 'note.whole-S1', 'note.whole-S2', 'note.whole-S3', 'note.whole-S4', 'note.whole-S5', 'note.whole-S6', 'note.whole-S7', 'rest.eighth-L3', 'rest.half-L3', 'rest.quadruple_whole-L3', 'rest.quarter-L3', 'rest.sixteenth-L3', 'rest.sixty_fourth-L3', 'rest.thirty_second-L3', 'rest.whole-L4', 'slur.end-L-1', 'slur.end-L-2', 'slur.end-L0', 'slur.end-L1', 'slur.end-L2', 'slur.end-L3', 'slur.end-L4', 'slur.end-L5', 'slur.end-L6', 'slur.end-L7', 'slur.end-L8', 'slur.end-S-1', 'slur.end-S0', 'slur.end-S1', 'slur.end-S2', 'slur.end-S3', 'slur.end-S4', 'slur.end-S5', 'slur.end-S6', 'slur.end-S7', 'slur.start-L-1', 'slur.start-L-2', 'slur.start-L0', 'slur.start-L1', 'slur.start-L2', 'slur.start-L3', 'slur.start-L4', 'slur.start-L5', 'slur.start-L6', 'slur.start-L7', 'slur.start-L8', 'slur.start-S-1', 'slur.start-S0', 'slur.start-S1', 'slur.start-S2', 'slur.start-S3', 'slur.start-S4', 'slur.start-S5', 'slur.start-S6', 'slur.start-S7']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5O3vMmIOgml"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "How can we measure how well an OMR system recognizes an input score? Unfortunately, this is a **long-standing unresolved question** in the field. \n",
        "\n",
        "However, in this context we only need a measure of how well the network is learning. The *loss* function value could be used but it is difficult to interpret. That is why we are going to use the **edit distance**.\n",
        "\n",
        "Given that our output domain consists of sequences of music symbols, we can *evaluate* the learning process as regards the edit distance between the ground-truth sequence and the sequence predicted by the model. \n",
        "\n",
        "The edit distance between *A* and *B* counts the number of editing operations (insert, delete, or replace) needed to convert a sequence *A* into the sequence *B*. Note that in our context this can be seen as the cost of correcting the output of the OMR system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72qIhgjaO7dT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9c210f8-3c27-4370-91ce-6e545afd001b"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "def edit_distance(a,b):\n",
        "    \"Computes the Levenshtein distance between sequence a and b.\"\n",
        "    n, m = len(a), len(b)\n",
        "\n",
        "    if n > m:\n",
        "        a,b = b,a\n",
        "        n,m = m,n\n",
        "\n",
        "    current = range(n+1)\n",
        "    for i in range(1,m+1):\n",
        "        previous, current = current, [i]+[0]*n\n",
        "        for j in range(1,n+1):\n",
        "            add, delete = previous[j]+1, current[j-1]+1\n",
        "            change = previous[j-1]\n",
        "            if a[j-1] != b[i-1]:\n",
        "                change = change + 1\n",
        "            current[j] = min(add, delete, change)\n",
        "\n",
        "    return current[n]\n",
        "  \n",
        "example_A = ['clef.F-L4', 'rest.quarter-L3', 'note.quarter-S3', 'barline-L1']\n",
        "example_B = ['clef.F-L4', 'rest.quarter-L1', 'note.quarter-S3', 'note.eighth-S2', 'barline-L1']\n",
        "\n",
        "print('Edit distance: ', edit_distance(example_A, example_B))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Edit distance:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSsy_EsmOQUk"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R836P1JDO6We"
      },
      "source": [
        "We now must define the generators (see above) for the training set, taking into account that:\n",
        "* The encoder receives an image\n",
        "* The decoder receives a sequence of symbols (starting with SOS)\n",
        "* The decoder emits a sequence of symbols (ending with EOS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfpyb6cE16IZ"
      },
      "source": [
        "\"\"\"\n",
        "We build a generator for getting batches of equal width size.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np # Python's Matrix package\n",
        "\n",
        "def vector_to_batch(X_batch, Y_batch):\n",
        "    max_batch_image_len = max([image.shape[1] for image in X_batch])\n",
        "\n",
        "    encoder_input = np.zeros((len(X_batch),img_height,max_batch_image_len), dtype=np.float)\n",
        "\n",
        "    for idx, image in enumerate(X_batch):\n",
        "        encoder_input[idx][:,:image.shape[1]] = image  \n",
        "\n",
        "    encoder_input = np.expand_dims(encoder_input, axis=-1) # Tensor-wise input (B,H,W) -> (B,H,W,1)\n",
        "    encoder_input = (255. - encoder_input) / 255. # Image normalization within [0,1]\n",
        "    \n",
        "    max_batch_output_len = max([len(sequence) for sequence in Y_batch])\n",
        "\n",
        "    decoder_input = np.zeros((len(Y_batch),max_batch_output_len,alphabet_len), dtype=np.float)\n",
        "    decoder_output = np.zeros((len(Y_batch),max_batch_output_len,alphabet_len), dtype=np.float)\n",
        "\n",
        "    for idx_s, output_sentence in enumerate(Y_batch):        \n",
        "        for idx_c, char in enumerate(output_sentence):\n",
        "            decoder_input[idx_s][idx_c][alphabet_from_char_to_int[char]] = 1.\n",
        "            if idx_c > 0:\n",
        "                decoder_output[idx_s][idx_c-1][alphabet_from_char_to_int[char]] = 1.\n",
        "                \n",
        "    return encoder_input, decoder_input, decoder_output\n",
        "\n",
        "def generator(X, Y, batch_size):\n",
        "  idx = 0\n",
        "  while True:\n",
        "    X_batch = X[idx:idx+batch_size]\n",
        "    Y_batch = Y[idx:idx+batch_size]\n",
        "    \n",
        "    encoder_input, decoder_input, decoder_output = vector_to_batch(X_batch, Y_batch)\n",
        "    \n",
        "    yield [encoder_input, decoder_input], decoder_output\n",
        "    \n",
        "    idx = (idx + batch_size) % len(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEzdg70dvKzu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1171933a-e112-4865-c5a5-c1bbf8e795cc"
      },
      "source": [
        "sanity_check_generator = generator(X,Y, batch_size = 16)\n",
        "\n",
        "[encoder_input, decoder_input], decoder_output = next(sanity_check_generator)\n",
        "\n",
        "print ('Encoder input: ' + str(encoder_input.shape))\n",
        "print ('Decoder input: ' + str(decoder_input.shape))\n",
        "print ('Decoder output: ' + str(decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder input: (16, 64, 699, 1)\n",
            "Decoder input: (16, 36, 706)\n",
            "Decoder output: (16, 36, 706)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo4705YSOyYs"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "We must select the specific hyper-parameters of our CNN-Encoder-Decoder. The example below sets the network as:\n",
        "* CNN with 4 layers\n",
        "* RNN-Encoder with one layer\n",
        "* RNN-Decoder with one layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWTOOMfrvqzr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "e15b8896-2de6-4102-824e-419e100951b4"
      },
      "source": [
        "from keras.layers import Input, Dense, Flatten, Reshape, Permute\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "def getmodel(img_height, alphabet_len):\n",
        "  filters = [32, 64, 128, 128]\n",
        "  w_pooling = [2,2,2,2]\n",
        "  h_pooling = [2,2,2,2]\n",
        "\n",
        "  if K.image_data_format() == 'channels_last':\n",
        "    input_data = Input(name='input', shape=(img_height, None, 1))\n",
        "  else:\n",
        "    input_data = Input(name='input', shape=(1, img_height, None))\n",
        "\n",
        "\n",
        "  # Encoder-CNN with 4 layers    \n",
        "  inner = Conv2D(filters[0], (3, 3), padding='same', activation='relu')(input_data)\n",
        "  inner = BatchNormalization()(inner)\n",
        "  inner = MaxPooling2D(pool_size=(h_pooling[0], w_pooling[0]))(inner)\n",
        "\n",
        "  inner = Conv2D(filters[1], (3, 3), padding='same', activation='relu')(inner)\n",
        "  inner = BatchNormalization()(inner)\n",
        "  inner = MaxPooling2D(pool_size=(h_pooling[1], w_pooling[1]))(inner)\n",
        "\n",
        "  inner = Conv2D(filters[2], (3, 3), padding='same', activation='relu')(inner)\n",
        "  inner = BatchNormalization()(inner)\n",
        "  inner = MaxPooling2D(pool_size=(h_pooling[2], w_pooling[2]))(inner)\n",
        "\n",
        "  inner = Conv2D(filters[3], (3, 3), padding='same', activation='relu')(inner)\n",
        "  inner = BatchNormalization()(inner)\n",
        "  inner = MaxPooling2D(pool_size=(h_pooling[3], w_pooling[3]))(inner)\n",
        "  \n",
        "  # Reshaping to input the Encoder-RNN\n",
        "  permute = Permute((2,1,3))(inner) #     \n",
        "  w_factor = np.prod(np.array(w_pooling))\n",
        "  h_factor = np.prod(np.array(h_pooling))\n",
        "\n",
        "  # This goes as a function of the number of poolings\n",
        "  conv_to_rnn_dims = (-1, (img_height // (h_factor)) * filters[3])        \n",
        "  encoder_input = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(permute)    \n",
        "\n",
        "  # Encoder-RNN\n",
        "  encoder_outputs, state_h, state_c = LSTM(512, return_state=True)(encoder_input)\n",
        "  encoder_states = [state_h, state_c]\n",
        "\n",
        "  # Decoder-RNN\n",
        "  decoder_inputs = Input(shape=(None, alphabet_len))\n",
        "  decoder_outputs, _, _ = LSTM(512, return_sequences=True, return_state=True)(decoder_inputs, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(alphabet_len, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  # We create the single model with two inputs \n",
        "  model = Model([input_data, decoder_inputs], decoder_outputs)\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy')   \n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "  \n",
        "model = getmodel(encoder_input.shape[1],alphabet_len)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              (None, 64, None, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 64, None, 32) 320         input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, None, 32) 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, None, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, None, 64) 18496       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, None, 64) 256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, None, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, None, 128 73856       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, None, 128 512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, None, 128) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 8, None, 128) 147584      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 8, None, 128) 512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 4, None, 128) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, None, 4, 128) 0           max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, None, 512)    0           permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, None, 706)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 512), (None, 2099200     reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 512),  2496512     input_1[0][0]                    \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 706)    362178      lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 5,199,554\n",
            "Trainable params: 5,198,850\n",
            "Non-trainable params: 704\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw1P-AGyOtCo"
      },
      "source": [
        "Data splitting into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygv0-sGswyYr"
      },
      "source": [
        "val_split = 0.1 # 10 % of the data will be used to validate the learning process\n",
        "idx_split = int(len(X)*val_split)\n",
        "\n",
        "X_train = X[idx_split:]\n",
        "Y_train = Y[idx_split:]\n",
        "\n",
        "X_val = X[:idx_split]\n",
        "Y_val = Y[:idx_split]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-ZiqaYjOvqK"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egnkkf7dw8LS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        },
        "outputId": "8012a549-5f48-4323-a588-d5cd3b081a7a"
      },
      "source": [
        "import math # Inf\n",
        "\n",
        "TR_BATCH_SIZE = 32\n",
        "train_generator = generator(X_train, Y_train, batch_size = TR_BATCH_SIZE)\n",
        "x_val, y_val, t_val = vector_to_batch(X_val, Y_val)\n",
        "\n",
        "# We monitor the edit distance over the validation set every two epochs\n",
        "MINI_EPOCH = 2\n",
        "previous_val_ed = math.inf\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(30):\n",
        "  print()\n",
        "  print('-----> Epoch', epoch*MINI_EPOCH)\n",
        "  \n",
        "  # Run the training process for some epochs\n",
        "  history = model.fit_generator(train_generator, \n",
        "                   steps_per_epoch=len(X_train)//TR_BATCH_SIZE, \n",
        "                   verbose=2, \n",
        "                   epochs=MINI_EPOCH, \n",
        "                   validation_data=[[x_val,y_val],t_val])  \n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  Monitorization over the validation set\n",
        "  \"\"\"\n",
        "  current_val_ed = 0\n",
        "  \n",
        "  # Prediction over the validation set\n",
        "  batch_prediction = model.predict([x_val,y_val],batch_size=16)\n",
        "  \n",
        "  # Loop over all validation samples\n",
        "  for idx,sentence_prediction in enumerate(batch_prediction):  \n",
        "\n",
        "      # Compute the ground-truth sequence\n",
        "      raw_gt_sequence = [alphabet_from_int_to_char[char] for char in np.argmax(t_val[idx],axis=1)] \n",
        "      gt_sequence = []          \n",
        "      \n",
        "      for char in raw_gt_sequence:\n",
        "        gt_sequence += [char]\n",
        "        if char == output_eos:\n",
        "            break\n",
        "      \n",
        "      # Compute the predicted sequence      \n",
        "      raw_predicted_sequence = [alphabet_from_int_to_char[char] for char in np.argmax(sentence_prediction,axis=1)] \n",
        "      predicted_sentence = []          \n",
        "      \n",
        "      for char in raw_predicted_sequence:\n",
        "        predicted_sentence += [char]\n",
        "        if char == output_eos:\n",
        "            break\n",
        "            \n",
        "      # Accumulate the edit distance\n",
        "      current_val_ed += edit_distance(gt_sequence,predicted_sentence)\n",
        "  \n",
        "  # Compute the average edit distance\n",
        "  current_val_ed = (1. * current_val_ed) / len(x_val)\n",
        "  \n",
        "  # Show validation results\n",
        "  print()\n",
        "  print()\n",
        "  print('Epoch ' + str(((epoch+1)*MINI_EPOCH)-1) + ' - Validation edit dist.: ' + str(current_val_ed))  \n",
        "  print()\n",
        "  print()\n",
        "  \n",
        "  \n",
        "  # To automatize the training and avoid overfitting, uncomment the following lines\n",
        "  '''\n",
        "  if current_val_ed > previous_val_ed:\n",
        "    break;\n",
        "  else:\n",
        "    previous_val_ed = current_val_ed\n",
        "  '''    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----> Epoch 0\n",
            "Epoch 1/2\n",
            " - 238s - loss: 0.9285 - val_loss: 0.7601\n",
            "Epoch 2/2\n",
            " - 237s - loss: 0.8548 - val_loss: 0.7459\n",
            "\n",
            "\n",
            "Epoch 1 - Validation edit dist.: 11.0585\n",
            "\n",
            "\n",
            "\n",
            "-----> Epoch 2\n",
            "Epoch 1/2\n",
            " - 238s - loss: 0.7830 - val_loss: 0.7077\n",
            "Epoch 2/2\n",
            " - 237s - loss: 0.7211 - val_loss: 0.7065\n",
            "\n",
            "\n",
            "Epoch 3 - Validation edit dist.: 10.4135\n",
            "\n",
            "\n",
            "\n",
            "-----> Epoch 4\n",
            "Epoch 1/2\n",
            " - 238s - loss: 0.6663 - val_loss: 0.6883\n",
            "Epoch 2/2\n",
            " - 237s - loss: 0.6164 - val_loss: 0.7155\n",
            "\n",
            "\n",
            "Epoch 5 - Validation edit dist.: 10.299\n",
            "\n",
            "\n",
            "\n",
            "-----> Epoch 6\n",
            "Epoch 1/2\n",
            " - 238s - loss: 0.5711 - val_loss: 0.6829\n",
            "Epoch 2/2\n",
            " - 237s - loss: 0.5293 - val_loss: 0.6783\n",
            "\n",
            "\n",
            "Epoch 7 - Validation edit dist.: 9.6995\n",
            "\n",
            "\n",
            "\n",
            "-----> Epoch 8\n",
            "Epoch 1/2\n",
            " - 238s - loss: 0.4905 - val_loss: 0.6879\n",
            "Epoch 2/2\n",
            " - 237s - loss: 0.4558 - val_loss: 0.6769\n",
            "\n",
            "\n",
            "Epoch 9 - Validation edit dist.: 9.646\n",
            "\n",
            "\n",
            "\n",
            "-----> Epoch 10\n",
            "Epoch 1/2\n",
            " - 238s - loss: 0.4250 - val_loss: 0.6970\n",
            "Epoch 2/2\n",
            " - 238s - loss: 0.3961 - val_loss: 0.7015\n",
            "\n",
            "\n",
            "Epoch 11 - Validation edit dist.: 9.598\n",
            "\n",
            "\n",
            "\n",
            "-----> Epoch 12\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNXHP98NuxJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "outputId": "cbb7085d-75c0-4ac8-b0f5-0585a487eadf"
      },
      "source": [
        "\"\"\"\n",
        "Example of prediction after the model is trained\n",
        "\"\"\"\n",
        "\n",
        "idx_eval = 1 # Id of the validation sample to test\n",
        "\n",
        "# Batch preparation\n",
        "sample_x, sample_y, _ = vector_to_batch([X_val[idx_eval]], [Y_val[idx_eval]])\n",
        "\n",
        "# Prediction and conversion to symbols\n",
        "batch_prediction = model.predict([sample_x, sample_y])\n",
        "predicted_sequence = [alphabet_from_int_to_char[char] for char in np.argmax(batch_prediction[0],axis=1)]\n",
        "\n",
        "# Plot score\n",
        "f = plt.figure()\n",
        "f.add_subplot(1,1,1)\n",
        "plt.imshow(X_val[idx_eval])\n",
        "plt.show(block=True)\n",
        "\n",
        "# Display hypothesis\n",
        "print('Hypothesis:')\n",
        "for symbol in predicted_sequence:\n",
        "  if symbol == output_eos:\n",
        "    break\n",
        "  \n",
        "  print('\\t',symbol)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwQAAACYCAYAAABaty5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngalAICIiIiKSwFQgEBERERFJYP8fTzghmN1sTrgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Hypothesis:\n",
            "\t clef.C-L1\n",
            "\t accidental.flat-L4\n",
            "\t accidental.flat-L2\n",
            "\t accidental.flat-S3\n",
            "\t digit.3-L4\n",
            "\t digit.4-L2\n",
            "\t digit.1-S5\n",
            "\t digit.1-S5\n",
            "\t multirest-L3\n",
            "\t barline-L1\n",
            "\t note.quarter-L4\n",
            "\t dot-S4\n",
            "\t dot-S2\n",
            "\t barline-L1\n",
            "\t barline-L1\n",
            "\t barline-L1\n",
            "\t note.eighth-S3\n",
            "\t note.eighth-L4\n",
            "\t rest.quarter-L3\n",
            "\t rest.quarter-L3\n",
            "\t barline-L1\n",
            "\t digit.1-S5\n",
            "\t dot-S6\n",
            "\t note.eighth-L5\n",
            "\t note.eighth-S4\n",
            "\t note.eighth-L4\n",
            "\t barline-L1\n",
            "\t note.eighth-S3\n",
            "\t barline-L1\n",
            "\t note.eighth-S3\n",
            "\t note.eighth-L3\n",
            "\t rest.quarter-L3\n",
            "\t rest.quarter-L3\n",
            "\t barline-L1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cuu4wVtreos"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "### Initialize your environment\n",
        "\n",
        "* Repeat the experiment in your own Google Colab\n",
        "  - The dataset is available [here](https://drive.google.com/file/d/1KOcLA-EBOSgbktFv3gzu1lsCPsy1AJ02/view?usp=sharing).\n",
        "\n",
        "### Basic\n",
        "* Hyper-parameter tuning for improving performance (both efficiency and effectiveness)\n",
        "* Implement *bucketting* \n",
        " \n",
        "### Medium\n",
        "* Error analysis: errors per symbol, errors per length of the image, errors per length of the output sequence\n",
        "* Make the RNNs bidirectional\n",
        "\n",
        "### Advanced\n",
        "* Convert the output to MIDI\n",
        "* Modifications for double output (glyph, position)  \n",
        "* Real prediction (the decoder input is unknown) \n",
        "* Add an attention model: [tutorial](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html)\n",
        "\n"
      ]
    }
  ]
}