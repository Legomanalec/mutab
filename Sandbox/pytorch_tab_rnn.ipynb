{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  37043\n",
      "Total Vocab:  117\n",
      "{'G19': 0, 'A04': 1, 'D12': 2, 'E06': 3, 'e15': 4, 'e17': 5, 'e16': 6, 'G10': 7, 'e18': 8, 'D00': 9, 'G11': 10, 'A15': 11, 'G06': 12, 'e00': 13, 'D10': 14, 'A01': 15, 'D01': 16, 'B08': 17, 'B07': 18, 'e12': 19, 'E17': 20, 'A02': 21, 'B04': 22, 'D06': 23, 'G08': 24, 'B13': 25, 'D15': 26, 'E09': 27, 'B03': 28, 'B09': 29, 'B02': 30, 'A14': 31, 'A12': 32, 'e08': 33, 'B01': 34, 'e14': 35, 'E07': 36, 'G15': 37, 'E03': 38, 'A11': 39, 'E19': 40, 'D17': 41, 'D04': 42, 'D14': 43, 'D05': 44, 'B12': 45, 'B17': 46, 'D09': 47, 'B06': 48, 'G16': 49, 'e01': 50, 'G09': 51, 'e07': 52, 'e02': 53, 'A10': 54, 'D11': 55, 'B20': 56, 'B10': 57, 'B15': 58, 'A05': 59, 'A06': 60, 'e11': 61, 'B14': 62, 'B18': 63, 'G18': 64, 'E14': 65, 'G21': 66, 'D07': 67, 'B11': 68, 'B00': 69, 'e04': 70, 'G04': 71, 'E10': 72, 'e05': 73, 'e09': 74, 'e21': 75, 'A03': 76, 'G17': 77, 'G03': 78, 'E11': 79, 'D16': 80, 'E08': 81, 'B16': 82, 'B19': 83, 'B21': 84, 'D02': 85, 'E00': 86, 'e06': 87, 'G12': 88, 'E04': 89, 'e19': 90, 'A07': 91, 'e13': 92, 'G02': 93, 'A08': 94, 'A09': 95, 'D03': 96, 'A00': 97, 'G00': 98, 'A13': 99, 'e03': 100, 'G01': 101, 'G07': 102, 'E12': 103, 'D08': 104, 'e20': 105, 'D13': 106, 'G05': 107, 'G14': 108, 'D19': 109, 'B05': 110, 'e10': 111, 'E02': 112, 'e22': 113, 'E05': 114, 'E01': 115, 'E13': 116}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "filename = \"../data/full_tab_string.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    " \n",
    "# create mapping of unique chars to integers\n",
    "chars = ['G19', 'A04', 'D12', 'E06', 'e15', 'e17', 'e16', 'G10', 'e18', \n",
    "          'D00', 'G11', 'A15', 'G06', 'e00', 'D10', 'A01', 'D01', 'B08', 'B07', \n",
    "          'e12', 'E17', 'A02', 'B04', 'D06', 'G08', 'B13', 'D15', 'E09', 'B03', \n",
    "          'B09', 'B02', 'A14', 'A12', 'e08', 'B01', 'e14', 'E07', 'G15', 'E03', \n",
    "          'A11', 'E19', 'D17', 'D04', 'D14', 'D05', 'B12', 'B17', 'D09', 'B06', \n",
    "          'G16', 'e01', 'G09', 'e07', 'e02', 'A10', 'D11', 'B20', 'B10', 'B15', \n",
    "          'A05', 'A06', 'e11', 'B14', 'B18', 'G18', 'E14', 'G21', 'D07', 'B11', \n",
    "          'B00', 'e04', 'G04', 'E10', 'e05', 'e09', 'e21', 'A03', 'G17', 'G03', 'E11', \n",
    "          'D16', 'E08', 'B16', 'B19', 'B21', 'D02', 'E00', 'e06', 'G12', 'E04', 'e19', \n",
    "          'A07', 'e13', 'G02', 'A08', 'A09', 'D03', 'A00', 'G00', 'A13', 'e03', 'G01', 'G07', \n",
    "          'E12', 'D08', 'e20', 'D13', 'G05', 'G14', 'D19', 'B05', 'e10', 'E02', 'e22', \n",
    "          'E05', 'E01', 'E13']\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    " \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text) // 4\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  9251\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 4):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_in = seq_in.split()\n",
    "\n",
    "    seq_out = raw_text[i + seq_length:i + seq_length+3]\n",
    "\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9251, 10, 1]) torch.Size([9251])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    " \n",
    " \n",
    "# reshape X to be [samples, time steps, features]\n",
    "mod_seq_length = seq_length // 4\n",
    "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, mod_seq_length, 1)\n",
    "X = X / float(n_vocab)\n",
    "y = torch.tensor(dataY)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    " \n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(256, n_vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 35123.4414\n",
      "Epoch 1: Cross-entropy: 35115.0547\n",
      "Epoch 2: Cross-entropy: 35048.6758\n",
      "Epoch 3: Cross-entropy: 35047.5781\n",
      "Epoch 4: Cross-entropy: 35021.4922\n",
      "Epoch 5: Cross-entropy: 34916.0508\n",
      "Epoch 6: Cross-entropy: 34814.4609\n",
      "Epoch 7: Cross-entropy: 34657.5469\n",
      "Epoch 8: Cross-entropy: 33527.3242\n",
      "Epoch 9: Cross-entropy: 33051.7109\n",
      "Epoch 10: Cross-entropy: 32259.2852\n",
      "Epoch 11: Cross-entropy: 31774.3086\n",
      "Epoch 12: Cross-entropy: 31850.6992\n",
      "Epoch 13: Cross-entropy: 30459.0195\n",
      "Epoch 14: Cross-entropy: 28976.2773\n",
      "Epoch 15: Cross-entropy: 27589.5938\n",
      "Epoch 16: Cross-entropy: 25601.4805\n",
      "Epoch 17: Cross-entropy: 24047.9961\n",
      "Epoch 18: Cross-entropy: 23041.7227\n",
      "Epoch 19: Cross-entropy: 22404.2988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "model = CharModel()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
    " \n",
    "torch.save([best_model, char_to_int], \"single-char.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 94, 95, 23, 104, 47, 12, 24]\n"
     ]
    }
   ],
   "source": [
    "tab_to_follow = ['A06', 'A08', 'A09', 'D06', 'D08', 'D09', 'G06', 'G08']\n",
    "tab_int = [char_to_int[char] for char in tab_to_follow]\n",
    "print(tab_int)\n",
    "input_tab = 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"60\"\n",
      "tensor([-0.3441, -1.2526, -0.8380])\n",
      "tensor(0)\n",
      "A06 tensor([-1.2390, -1.6690, -0.8857])\n",
      "tensor(2)\n",
      "D03 tensor([-0.8438, -0.7367, -0.2894])\n",
      "tensor(2)\n",
      "D04 tensor([-0.3261, -0.2557, -1.6228])\n",
      "tensor(1)\n",
      "A11 tensor([-0.0401, -1.0427, -0.4801])\n",
      "tensor(0)\n",
      "D08 tensor([ 0.2313, -1.0570, -0.2214])\n",
      "tensor(0)\n",
      "D09 tensor([-0.3017, -0.2318,  0.1552])\n",
      "tensor(2)\n",
      "B02 tensor([ 0.5206, -1.3698, -0.5225])\n",
      "tensor(0)\n",
      "G08 Done.\n"
     ]
    }
   ],
   "source": [
    "Eb = ['A06', 'E11', 'D01']\n",
    "F = ['A08', 'E13', 'D03']\n",
    "Gb = ['A09', 'E14', 'D04']\n",
    "Ab = ['D06', 'A11', 'G01']\n",
    "Bb = ['D08', 'A13', 'G03']\n",
    "Cb = ['D09', 'A14', 'G04']\n",
    "Db = ['G06', 'D11', 'B02']\n",
    "Eb2 = ['G08', 'D13', 'B04']\n",
    "\n",
    "tab_int_set = []\n",
    "tab_int_set.append([char_to_int[char] for char in Eb])\n",
    "tab_int_set.append([char_to_int[char] for char in F])\n",
    "tab_int_set.append([char_to_int[char] for char in Gb])\n",
    "tab_int_set.append([char_to_int[char] for char in Ab])\n",
    "tab_int_set.append([char_to_int[char] for char in Bb])\n",
    "tab_int_set.append([char_to_int[char] for char in Cb])\n",
    "tab_int_set.append([char_to_int[char] for char in Db])\n",
    "tab_int_set.append([char_to_int[char] for char in Eb2])\n",
    "\n",
    "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "\n",
    "model.eval()\n",
    "print('Prompt: \"%s\"' % input_tab)\n",
    "with torch.no_grad():\n",
    "    for i in range(8):\n",
    "        # format input array of int into PyTorch tensor\n",
    "        x = np.reshape(tab_int_set[i], (1, len(tab_int_set[i]), 1)) / float(n_vocab)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        # generate logits as output from the model\n",
    "        prediction = model(x)\n",
    "\n",
    "        indices = torch.tensor(tab_int_set[i])\n",
    "        torch.index_select(prediction, 1, indices)\n",
    "\n",
    "        pred_set = []\n",
    "        pred_set.append(prediction[0][tab_int_set[i][0]])\n",
    "        pred_set.append(prediction[0][tab_int_set[i][1]])\n",
    "        pred_set.append(prediction[0][tab_int_set[i][2]])\n",
    "        pred_set = torch.Tensor(pred_set)\n",
    "        print(pred_set)\n",
    "        print(pred_set.argmax())\n",
    "        # convert logits into one character\n",
    "        index = int(tab_int_set[i][pred_set.argmax()])\n",
    "        result = int_to_char[index]\n",
    "        print(result, end=\" \")\n",
    "        # append the new character into the prompt for the next iteration\n",
    "        tab_int_set[i].append(index)\n",
    "        tab_int_set[i] = tab_int_set[i][1:]\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
