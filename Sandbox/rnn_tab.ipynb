{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "with open('../data/full_tab_string.txt') as small_pf:\n",
    "\n",
    "    tmp_list = []\n",
    "    for line in small_pf:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if line == \"\":\n",
    "            lst.append(tmp_list)\n",
    "            tmp_list = []\n",
    "        else:\n",
    "            tmp_list.extend(line.split())\n",
    "\n",
    "    if tmp_list:  # add last one\n",
    "        lst.append(tmp_list)\n",
    "lst = lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open('../data/full_tab_string.txt') as small_pf:\n",
    "    text = small_pf.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can't use set because tab encodings are not hashable types\n",
    "def unique(list1):\n",
    "    unique_set = set()\n",
    "    for x in list1:\n",
    "        if x not in unique_set:\n",
    "            unique_set.add(x)\n",
    "    return unique_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all tab locations\n",
    "tab_vocab = unique(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'G3', 'B22', 'E3', 'B11', 'G5', 'e22', 'A9', 'G19', 'D2', 'G21', 'E4', 'D16', 'E14', 'D7', 'G14', 'G17', 'D6', 'B0', 'e3', 'A4', 'G81', 'D19', 'D4', 'D14', 'E2', 'G11', 'B2', 'B18', 'e18', 'E13', 'B14', 'G16', 'A0', 'E15', 'E6', 'e2', 'e7', 'E11', 'B5', 'B4', 'e15', 'A8', 'D8', 'G6', 'D13', 'G4', 'A16', 'B15', 'B7', 'B3', 'A12', 'A17', 'E8', 'e23', 'B9', 'A15', 'e4', 'E19', 'A5', 'D15', 'D5', 'B1', 'A10', 'A7', 'e8', 'e19', 'B13', 'A19', 'D11', 'D12', 'D17', 'e24', 'B6', 'A2', 'A3', 'A6', 'e13', 'e16', 'B24', 'e1', 'e10', 'G2', 'G1', 'G8', 'A11', 'G12', 'E17', 'e20', 'G13', 'B17', 'D1', 'E10', 'e12', 'B19', 'D10', 'B20', 'A13', 'e5', 'G10', 'G0', 'E0', 'B12', 'e0', 'e14', 'A14', 'G9', 'B16', 'A1', 'e9', 'D3', 'e6', 'B8', 'A33', 'G7', 'E5', 'D9', 'e11', 'e17', 'E1', 'E9', 'B21', 'B10', 'G18', 'E12', 'D0', 'e21', 'E7', 'G15']\n"
     ]
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    num_oov_indices = 1, \n",
    "    vocabulary=list(tab_vocab), \n",
    "    mask_token=None)\n",
    "print(ids_from_chars.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids) + \" \", axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.from_tensor_slices_op.TensorSliceDataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "type(ids_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D7\n",
      "G5\n",
      "B5\n",
      "e5\n",
      "e7\n",
      "D6\n",
      "B5\n",
      "G5\n",
      "e7\n",
      "e8\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "tf.Tensor(\n",
      "[b'D7' b'G5' b'B5' b'e5' b'e7' b'D6' b'B5' b'G5' b'e7' b'e8' b'D5' b'B5'\n",
      " b'G5' b'e8' b'e2' b'D4' b'B3' b'G2' b'e2' b'e0' b'D3'], shape=(21,), dtype=string)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "print(type(sequences))\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))\n",
    "  print(type(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'D7 G5 B5 e5 e7 D6 B5 G5 e7 e8 D5 B5 G5 e8 e2 D4 B3 G2 e2 e0 D3 '\n",
      "b'B1 G2 B1 e0 B1 G2 B0 G0 A2 B1 G2 A0 B1 G2 A0 G2 A0 A8 A7 A0 D7 '\n",
      "b'G5 B5 e7 D6 B5 G5 e7 e8 D5 B5 G5 e8 e2 D4 B3 G2 e2 e0 D3 B1 G2 '\n",
      "b'B1 e0 B1 G2 B0 G0 A2 B1 G2 A0 B1 G2 A0 A0 A2 A3 D2 G0 e0 e2 D0 '\n",
      "b'B3 G2 e2 e0 D3 B1 G2 e0 B0 A0 B1 G2 A0 A2 B1 A3 G0 D2 B1 e3 E3 '\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BatchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4416\\3160607509.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msplit_input_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4416\\557121777.py\u001b[0m in \u001b[0;36msplit_input_target\u001b[1;34m(sequence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_input_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtarget_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'BatchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "split_input_target(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(20,), dtype=tf.int64, name=None), TensorSpec(shape=(20,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'D7 G5 B5 e5 e7 D6 B5 G5 e7 e8 D5 B5 G5 e8 e2 D4 B3 G2 e2 e0 '\n",
      "Target: b'G5 B5 e5 e7 D6 B5 G5 e7 e8 D5 B5 G5 e8 e2 D4 B3 G2 e2 e0 D3 '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 20), dtype=tf.int64, name=None), TensorSpec(shape=(64, 20), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "\n",
    "    #adjust the dense units size                         \n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20, 129) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  33024     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  132225    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,103,553\n",
      "Trainable params: 4,103,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 99, 127,  73,  43,  90, 116,  16, 123,   1,  48, 117,   3,  73,\n",
       "         9,  32,  84, 115,  32,  66,  51], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'B12 B0 B10 B0 B9 B0 B10 B0 G4 B9 B0 B7 B0 B9 B0 B5 B0 B7 B0 B4 '\n",
      "\n",
      "Next Char Predictions:\n",
      " b'G10 E7 B6 D8 B17 D9 G17 G18 G3 B15 e11 E3 B6 D2 G16 G8 E5 G16 e19 A12 '\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 20, 129)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.8617597, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './tab_training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.2883\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 0.2793\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 0.2723\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.2685\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 0.2623\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.2586\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 0.2545\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 0.2511\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 0.2471\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 0.2439\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 0.2408\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 0.2398\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 0.2377\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 0.2354\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 0.2336\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 0.2325\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 0.2294\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 8s 234ms/step - loss: 0.2282\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 0.2261\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.2256\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 0.2254\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 0.2254\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 0.2227\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 0.2206\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.2210\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.2221\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - 8s 233ms/step - loss: 0.2188\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.2197\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.2185\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - 8s 234ms/step - loss: 0.2170\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.2169\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 0.2171\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 0.2158\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.2165\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 0.2160\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.2166\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.2152\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.2140\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.2141\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.2135\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "\n",
    "    #USE THIS TO CHANGE WHICH OUTPUTS ARE ALLOWED\n",
    "    skip_ids = self.ids_from_chars(['[UNK]', 'G3', 'B22', 'E3', 'B11', \n",
    "    'G5', 'e22', 'A9', 'G19', 'D2', 'G21', 'E4', 'E14', 'D7', \n",
    "    'G14', 'G17', 'D6', 'B0', 'e3', 'A4', 'G81', 'D19', 'D4', 'D14', \n",
    "    'E2', 'B2', 'B18', 'e18', 'E13', 'B14', 'G16', 'A0', 'E15', \n",
    "    'E6', 'e7', 'E11', 'B5', 'B4', 'e15', 'A8', 'D8', 'G6', 'D13', \n",
    "    'G4', 'A16', 'B15', 'B3', 'A12', 'A17', 'E8', 'e23', 'B9', \n",
    "    'A15', 'e4', 'E19', 'A5', 'D15', 'D5', 'B1', 'A10', 'A7', 'e8', \n",
    "    'e19', 'B13', 'A19', 'D11', 'D12', 'D17', 'e24', 'B6', 'A2', 'A3', \n",
    "    'A6', 'e13', 'e16', 'B24', 'e1', 'e10', 'G2', 'G1', 'G8', 'A11', \n",
    "    'G12', 'E17', 'e20', 'G13', 'B17', 'D1', 'E10', 'e12', 'B19', 'D10',\n",
    "     'B20', 'A13', 'e5', 'G10', 'G0', 'E0', 'B12', 'e0', 'e14', \n",
    "     'A14','G9', 'B16', 'A1', 'e9', 'D3', 'e6', 'B8', 'A33', 'G7', \n",
    "     'E5', 'D9', 'e11', 'e17', 'E1', 'E9', 'B21', 'B10', 'G18', 'E12', \n",
    "     'D0', 'e21', 'E7', 'G15'])[:, None]\n",
    "\n",
    "    # skip_ids = self.ids_from_chars('[UNK]', 'G3', 'B22', 'E3', 'B11', \n",
    "    # 'G5', 'e22', 'A9', 'G19', 'D2', 'G21', 'E4', 'D16', 'E14', 'D7', \n",
    "    # 'G14', 'G17', 'D6', 'B0', 'e3', 'A4', 'G81', 'D19', 'D4', 'D14', \n",
    "    # 'E2', 'G11', 'B2', 'B18', 'e18', 'E13', 'B14', 'G16', 'A0', 'E15', \n",
    "    # 'E6', 'e2', 'e7', 'E11', 'B5', 'B4', 'e15', 'A8', 'D8', 'G6', 'D13', \n",
    "    # 'G4', 'A16', 'B15', 'B7', 'B3', 'A12', 'A17', 'E8', 'e23', 'B9', \n",
    "    # 'A15', 'e4', 'E19', 'A5', 'D15', 'D5', 'B1', 'A10', 'A7', 'e8', \n",
    "    # 'e19', 'B13', 'A19', 'D11', 'D12', 'D17', 'e24', 'B6', 'A2', 'A3', \n",
    "    # 'A6', 'e13', 'e16', 'B24', 'e1', 'e10', 'G2', 'G1', 'G8', 'A11', \n",
    "    # 'G12', 'E17', 'e20', 'G13', 'B17', 'D1', 'E10', 'e12', 'B19', 'D10',\n",
    "    #  'B20', 'A13', 'e5', 'G10', 'G0', 'E0', 'B12', 'e0', 'e14', \n",
    "    #  'A14', 'G9', 'B16', 'A1', 'e9', 'D3', 'e6', 'B8', 'A33', 'G7', \n",
    "    #  'E5', 'D9', 'e11', 'e17', 'E1', 'E9', 'B21', 'B10', 'G18', 'E12', \n",
    "    #  'D0', 'e21', 'E7', 'G15')\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D9 D11 G9 e2 \n"
     ]
    }
   ],
   "source": [
    "#get more than 3 Epochs to decrease loss for better predictions. This is, i believe the perfect basis \n",
    "#for getting tab to work.\n",
    "histthis = []\n",
    "for i in range(100):\n",
    "  states = None\n",
    "  next_char = tf.constant(['D9 D11 G9'])\n",
    "  result = [next_char + \" \"]\n",
    "  for n in range(1):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char + \" \")\n",
    "\n",
    "  result = tf.strings.join(result)\n",
    "\n",
    "  if result[0].numpy().decode('utf-8') == 'D9 D11 G9 G11 ':\n",
    "    histthis.append(0)\n",
    "  elif result[0].numpy().decode('utf-8') == 'D9 D11 G9 B7 ':\n",
    "    histthis.append(1)\n",
    "  elif result[0].numpy().decode('utf-8') == 'D9 D11 G9 e2 ':\n",
    "    histthis.append(2)\n",
    "  elif result[0].numpy().decode('utf-8') == 'D9 D11 G9 d16 ':\n",
    "    histthis.append(3)\n",
    "print(result[0].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histthis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to be successfully predicting, but the fact that it predicts an e2 here consistantly is not good. Will need to clean things up and check what is going on. I could be testing things wrong. Not sure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([26.,  0.,  0.,  0.,  0., 12.,  0.,  0.,  0., 61.]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(histthis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6ddfb2dcac28d85c50c41a252d540a7fa8764b66ffa4029a84ab1fab305572"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
